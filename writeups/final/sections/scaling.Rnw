\SweaveOpts{echo=F,cache=T,external=T}

<<cache=F>>=
library(ggplot2)

source("../writeup_base.R")
@

\section{\label{sec:scaling}Scaling to 9x9 Go}

Go has long been a challenge domain for AI 
\citep{cai2007computer,bouzy2001computer}, with the best computer
Go players only recently able to play at near-professional level. While 
grandmaster play by computers has been achieved in the challenging games of 
Chess and Backgammon, the approaches used to address these domains have proven
ineffective for Go. The high branching factor of the Go game tree yields \
classical planning techniques like minimax search intractable. Furthermore, it 
has proven difficult to hand-craft a good evaluation heuristic for the state 
of a board as has be done with other board games. 

Despite its challenges, Go has many appealing properties for studying
representation discovery in a large domain. It is a discrete game with simple,
deterministic rules, making simulation cheap and effective. Also, the best 
handcrafted programs have only proven moderately successful, making automatic 
methods for generating a representation appealing.

Go programs have become much stronger recently with the introduction of 
Monte Carlo Tree Search algorithms to the field.

% XXX figure for sampled paths in TTT gameplay graph

XXX

\begin{figure}
\begin{center}
%\includegraphics[width=\textwidth]{results/go_graph.pdf}
\end{center}
\caption{\label{fig:go.hairball}XXX}
\end{figure}

\begin{figure}
<<go.regression,fig=T,height=2.5>>=
data <- read.csv("../results/go_prediction.csv")
data$provenance <-
    factor(
        data$provenance,
        levels = c("on_graph", "off_graph"),
        labels = c("On-Graph", "Off-Graph"))
aesthetic <-
    aes(features, score_mean, colour = map_name, shape = map_name, ymin = score_mean - score_variance, ymax = score_mean + score_variance)
plot <-
    ggplot(data[data$samples < 20000,], aesthetic) +
    geom_point() +
    geom_smooth(stat = "identity") +
    facet_grid(provenance ~ samples) +
    labs(x = "Number of Features", y = "Mean $R^2$", colour = "Feature Set", shape = "Feature Set")

print(plot)
@
\caption{\label{fig:go.regression}The mean $R^2$ score of value function
prediction versus the number of feature vectors added from the specified
feature set, under $10$-fold cross validation using ridge regression ($\alpha =
1.0$) in the Go domain. As in TTT, these features are added to the set of raw,
flattened-grid features used to construct the affinity graph. Graphs of three
different sizes are evaluated, and the test states are drawn either from the
set of the graph vertices (``On-Graph'') or from a held-out set (``Off-Graph'')
whose features are then computed by linearly interpolating those of their $8$
nearest neighbors. Interpolation has the effect of smoothing the graph, giving
random features some minimal ability to generalize, but spectral features are
clearly much more useful. The limiting factor appears to be the computational
cost of computing additional eigenvectors.}
\end{figure}

% XXX cite Fuego

