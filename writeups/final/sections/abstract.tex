\begin{abstract}

The engineering of an appropriate state space representation is one of the
essential prerequisites to the use of reinforcement learning (RL) in a new
domain. Constructing a successful representation, however, is often difficult
and requires an understanding of the peculiarities of a given domain. Recent
work of \citet{Mahadevan2006Value}, among others, has applied techniques from
spectral graph theory to the problem of automatically discovering or augmenting
state representations, but only to small and amenable domains. This work
extends those techniques to tackle much larger domains, focusing on the unique
challenges of board games. It considers three related problems: \emph{applying}
spectral methods to the unusual tree-like state graphs of such games,
\emph{adapting} these methods to acquire state information from sampled expert
gameplay, and \emph{scaling} them to combinatorial state spaces. The approach
is verified on the simple game of Tic-Tac-Toe, before applying it to the vastly
more difficult game of Go. In both domains, spectral analysis proves able to
automatically generate features relevant to value function prediction.

\end{abstract}

