\begin{abstract}

The engineering of an appropriate state space representation is one of the
essential prerequisites to the use of reinforcement learning (RL) in a new
domain. Constructing a successful representation, however, is often difficult
and requires an understanding of the peculiarities of a given domain. Recent
work of \citet{Mahadevan2006Value}, among others, has applied techniques from
spectral graph theory to the problem of automatically discovering or augmenting
state representations, but only to small and amenable domains. This paper
extends those techniques to tackle larger, more challenging domains. It
examines three related problems: \emph{applying} spectral methods to the
unusual tree-like state graphs of board games, \emph{adapting} these methods to
acquire state information from sampled human gameplay, and \emph{scaling} them
to combinatorial state spaces. The simple game of Tic-Tac-Toe is used for
small-scale exploration, and the classic game of Go is used to examine behavior
in increasingly large domains.

\end{abstract}

