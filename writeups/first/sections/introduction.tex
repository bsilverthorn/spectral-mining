\section{Introduction}

Reinforcement learning (RL) is concerned with situations where an agent is
trying to learn from interactions with an environment. Value-based approaches 
to RL try to estimate the value function across the state space of the 
environment, and typically use that value function to produce a value-greedy
policy.

Just as feature engineering is often critical to the success of supervised 
learning methods, the success of value-based RL depends heavily on the quality
of the represenation used to estimate the value function. This work aims to 
explore methods for automatically generating such a represenation with a focus 
on approaches that scale to large discrete domains.

\subsection{Linear Value Function Approximation}

In environments where the number of states is too large to enumerate and store 
a tabular representation of the value function, some form of approximation must
be used. For simplicity and ease of analysis, a linear architecture is often 
chosen. In such a representation, the value of a state $s$ is estimated as a 
linear combination of features $\phi(s)$ of that state.

\begin{equation}
V(s) = w^{T}\phi(s)
\end{equation}

As a linear architecture is very concise, the quality of the resulting value
function and the performance of the resulting policy depends critically on the 
basis functions or state features $\phi(s)$. These features are typically 
constructed by hand, which can be a very labor-intensive process and require
extensive domain knowledge. 

The goal of this work is to leverage results from spectral graph theory to aid 
in discovering useful representations for linear value function approximation
in reinforcement learning. 

\subsection{Spectral Representation discovery}

Spectral graph theory provides many useful tools for performing analysis of a 
graph and functions on that graph \citep{TODO-chung}. The graph Laplacian $L$ 
and its normalized variants play a central role in the harmonic analysis of 
graphs. Given a weighted graph $W$, the graph laplacian is defined as:

\begin{equation}
L = D-W
\end{equation}

Where $D$ is defined as as the row sums of $W$:

\begin{equation}
D = \sum_{i} W_{ij}
\end{equation}

The normalized graph laplacian is defined to be

\begin{equation}
\mathcal{L} = D^{-1/2}LD^{-1/2}
\end{equation}

A particularly ineteresting property of the graph Laplacian is that is that the
eigenvectors form a good basis for representing smooth functions on the graph
\citep{TODO-needed?}.

The state space of a finite Markov Decision Process (MDP) can be represented
as a graph where each vertex is a state and edges represent state transitions.
Using this state adjacency graph $W$, we can form the (normalized) graph 
Laplacian and use the eigenvectors as a basis for performing linear value 
function approximation.

Recent work \citep{Wang2009Multiscale,Mahadevan2006Value,Coifman06Diffusion} has 
begun to examine the use of harmonic analysis of graphs in this framework, but 
have primarily focused on domains that are small enough for direct application 
of the approaches described above. However, as the state space grows large, 
representing the full adjacency graph and finding the eigenvalues becomes 
prohibitively expensive. 

This work examines the case where we do not have direct access to the full state
space, only sample trajectories. These samples are often disjoint, forming long
isolated chains, and may not provide a rich enough topology to apply spectral 
representation discovery techniques directly. Additionally we explore approaches
that can be used when finding eigenvalues, even on the sampled Laplacian matrix,
is computationally infeasible. We show that clustering as a preprocessing step
may be a viable remedy to both problems.

We focus on three domains: first, a simple two-room grid world to display the 
desirable properties that the Laplacian eigenvectors have in terms of adapting 
the representation to the topology of the state space; second, we explore the 
game of tic-tac-toe as a toy domain where we can analyze the effects of certain
approximation techniques in comparison with optimal and full-information 
approaches; and third, we choose the board game Go as a full-scale application
domain.

% XXX make sure we mention which Laplacian we're using

