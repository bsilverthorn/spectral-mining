@inproceedings{Wang2009Multiscale,
    abstract = {{We introduce a nonparametric approach to multiscale analysis of document corpora using a hierarchical matrix analysis framework called diffusion wavelets. In contrast to eigenvector methods, diffusion wavelets construct multiscale basis functions. In this framework, a hierarchy is automatically constructed by an iterative series of dilation and orthogonalization steps beginning with an initial set of orthogonal basis functions, such as the unitvector bases. Each set of basis functions at a given level is constructed from the bases at the lower level by dilation using the dyadic powers of a diffusion operator. A novel aspect of our work is that the diffusion analysis is conducted on the space of variables (words), instead of instances (documents). This approach can automatically and efficiently determine the number of levels of the topical hierarchy, as well as the topics at each level. Multiscale analysis of document corpora is achieved by using the projections of the documents onto the spaces spanned by basis functions at different levels. Further, when the input term-term matrix is a "local" diffusion operator, the algorithm runs in time approximately linear in the number of non-zero elements of the matrix. The approach is illustrated on various data sets including NIPS conference papers, 20 Newsgroups and TDT2 data.}},
    author = {Wang, Chang and Mahadevan, Sridhar},
    booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI)},
    citeulike-article-id = {8050459},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1661701},
    comment = {(private-note)Printed.},
    posted-at = {2011-09-04 18:40:17},
    priority = {2},
    title = {{Multiscale analysis of document corpora based on diffusion models}},
    year = {2009}
}

@article{Mahadevan2006Value,
    author = {Mahadevan, Sridhar and Maggioni, Mauro},
    citeulike-article-id = {9738310},
    comment = {(private-note)Printed.},
    journal = {Advances in Neural Information Processing Systems (NIPS)},
    posted-at = {2011-09-04 18:48:38},
    priority = {2},
    publisher = {Citeseer},
    title = {Value function approximation with diffusion wavelets and {L}aplacian eigenfunctions},
    year = {2006}
}

@article{Coifman06Diffusion,
    abstract = {{Our goal in this paper is to show that many of the tools of signal processing, adapted Fourier and wavelet analysis can be naturally lifted to the setting of digital data clouds, graphs, and manifolds.We use diffusion as a smoothing and scaling tool to enable coarse graining and multiscale analysis. Given a diffusion operator T on a manifold or a graph, with large powers of low rank, we present a general multiresolution construction for efficiently computing, representing and compressing T^t . This allows a direct multiscale computation, to high precision, of functions of the operator, notably the associated Green's function, in compressed form, and their fast application. Classes of operators for which these computations are fast include certain diffusion-like operators, in any dimension, on manifolds, graphs, and in non-homogeneous media. We use ideas related to the Fast Multipole Methods and to the wavelet analysis of Calder\'{o}n–Zygmund and pseudo-differential operators, to numerically enforce the emergence of a natural hierarchical coarse graining of a manifold, graph or data set. For example for a body of text documents the construction leads to a directory structure at different levels of generalization. The dyadic powers of an operator can be used to induce a multiresolution analysis, as in classical Littlewood–Paley and wavelet theory: we construct, with efficient and stable algorithms, bases of orthonormal scaling functions and wavelets associated to this multiresolution analysis, together with the corresponding downsampling operators, and use them to compress the corresponding powers of the operator. While most of our discussion deals with symmetric operators and relates to localization to spectral bands, the symmetry of the operators and their spectral theory need not be considered, as the main assumption is reduction of the numerical ranks as we take powers of the operator.}},
    author = {Coifman, R. and Maggioni, M.},
    citeulike-article-id = {7855687},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.acha.2006.04.004},
    citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S106352030600056X},
    journal = {Applied and Computational Harmonic Analysis},
    keywords = {diffusion, graph, high\_dimensional, manifold, nystrom\_extension, signal\_processing, wavelet},
    posted-at = {2010-09-19 15:08:11},
    priority = {0},
    title = {{Diffusion wavelets}},
    year = {2006}
}

@ARTICLE{Dhillon07weightedgraph,
    author = {Inderjit S. Dhillon and Yuqiang Guan and Brian Kulis},
    title = {Weighted graph cuts without eigenvectors : a multilevel approach},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    year = {2007}
}

@inproceedings{Silver2007Shape,
 author = {Silver, David and Sutton, Richard and M\"{u}ller, Martin},
 title = {Reinforcement learning of local shape in the game of {G}o},
 booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence (IJCAI)},
 year = {2007}
} 

@book{Bellman1957Dynamic,
    author = {Bellman, Richard},
    title = {Dynamic programming},
    year = {1957}
}

@book{Chung1997Spectral,
  title={Spectral graph theory},
  author={Chung, F.R.K.},
  year={1997}
}

@techreport{Omohundro89Five,
    author = {Stephen M. Omohundro},
    title = {Five balltree construction algorithms},
    institution = {},
    year = {1989}
}

@article{whiteson2006evolutionary,
  title={Evolutionary function approximation for reinforcement learning},
  author={Whiteson, S. and Stone, P.},
  journal={The Journal of Machine Learning Research},
  volume={7},
  pages={877--917},
  year={2006},
  publisher={JMLR. org}
}

@inproceedings{parr2007analyzing,
  title={Analyzing feature generation for value-function approximation},
  author={Parr, R. and Painter-Wakefield, C. and Li, L. and Littman, M.},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={737--744},
  year={2007},
  organization={ACM}
}

@inproceedings{petrik2007analysis,
  title={An analysis of Laplacian methods for value function approximation in MDPs},
  author={Petrik, M.},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  pages={2574--2579},
  year={2007}
}

@inproceedings{mahadevan2006learning,
  title={Learning representation and control in continuous Markov decision processes},
  author={Mahadevan, S. and Maggioni, M. and Ferguson, K. and Osentoski, S.},
  booktitle={Proceedings of the National Conference on Artificial Intelligence},
  volume={21},
  number={2},
  pages={1194},
  year={2006},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}

@article{mahadevan2007proto,
  title={Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes},
  author={Mahadevan, S. and Maggioni, M.},
  journal={Journal of Machine Learning Research},
  volume={8},
  pages={2169--2231},
  year={2007}
}

@inproceedings{johns2007compact,
  title={Compact spectral bases for value function approximation using Kronecker factorization},
  author={Johns, J. and Mahadevan, S. and Wang, C.},
  booktitle={PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE},
  volume={22},
  number={1},
  pages={559},
  year={2007},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}

@article{chung2005laplacians,
  title={Laplacians and the Cheeger inequality for directed graphs},
  author={Chung, F.},
  journal={Annals of Combinatorics},
  volume={9},
  number={1},
  pages={1--19},
  year={2005},
  publisher={Springer}
}




